{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "884e067c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读入数据\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "path = r\"326312325_按序号_决策与价值判断调查_150_106.xlsx\"\n",
    "data = pd.read_excel(path)\n",
    "\n",
    "# ——（可选）若列名可能有前后空格/隐藏字符，做一次标准化——\n",
    "data.columns = [c.strip() for c in data.columns]\n",
    "\n",
    "# 找到“注意力题”列名（容错：包含“这题请选1”的那一列）\n",
    "attn_col = None\n",
    "for c in data.columns:\n",
    "    if \"这题请选1\" in c:\n",
    "        attn_col = c\n",
    "        break\n",
    "if attn_col is None:\n",
    "    raise KeyError(\"未找到包含‘这题请选1’的注意力题列，请检查表头。\")\n",
    "\n",
    "# 过滤注意力题=1 的样本，并copy防止链式赋值\n",
    "data = data[pd.to_numeric(data[attn_col], errors=\"coerce\") == 1].copy()\n",
    "\n",
    "# 把 0-100 的概率答案安全地映射到 [0,1]，带越界裁剪与非数值处理\n",
    "def prob01(df, colname):\n",
    "    # 转为数值，非法值->NaN；裁剪到0-100；再除以100到0-1\n",
    "    return pd.to_numeric(df[colname], errors=\"coerce\").clip(0, 100) / 100.0\n",
    "\n",
    "s_1 = prob01(data, \"7、假设你做出了以下选择，你认为问题被解决的概率有多大，请填写0-100之间的数值—直接举报\")\n",
    "s_2 = prob01(data, \"7、匿名举报\")\n",
    "s_3 = prob01(data, \"7、继续保持观察，等待时机或者搜集更多证据后再采取行动\")\n",
    "s_4 = prob01(data, \"7、保持沉默，等待第三方想办法解决问题\")\n",
    "\n",
    "c_1 = prob01(data, \"8、假设你做出了以下选择，你认为你有多大概率会承受难以承受的损失，请填写0-100之间的数值—直接举报\")\n",
    "c_2 = prob01(data, \"8、匿名举报\")\n",
    "c_3 = prob01(data, \"8、继续保持观察，等待时机或者搜集更多证据后再采取行动\")\n",
    "c_4 = prob01(data, \"8、保持沉默，等待第三方想办法解决问题\")\n",
    "\n",
    "df_s_c = pd.DataFrame({\n",
    "    \"s_1\": s_1, \"s_2\": s_2, \"s_3\": s_3, \"s_4\": s_4,\n",
    "    \"c_1\": c_1, \"c_2\": c_2, \"c_3\": c_3, \"c_4\": c_4\n",
    "})\n",
    "\n",
    "# 基本健诊：确认全部在[0,1]（允许NaN）\n",
    "assert ((df_s_c >= 0) & (df_s_c <= 1) | df_s_c.isna()).all().all(), \"存在超出[0,1]的概率值\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "37c19b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_prob_columns(df_s_c, s_col, c_col, i, check_sum=True, tol=1e-9):\n",
    "    s = pd.to_numeric(df_s_c[s_col], errors=\"coerce\")\n",
    "    c = pd.to_numeric(df_s_c[c_col], errors=\"coerce\")\n",
    "    mask = s.notna() & c.notna()\n",
    "\n",
    "    # 先创建 NaN 列，防止把无效行写成0\n",
    "    for k in range(1, 5):\n",
    "        df_s_c[f\"p{i}_{k}\"] = np.nan\n",
    "\n",
    "    # 仅对有效行计算\n",
    "    s_m = s[mask]\n",
    "    c_m = c[mask]\n",
    "    df_s_c.loc[mask, f\"p{i}_1\"] = s_m * c_m                  # 成功 & 代价\n",
    "    df_s_c.loc[mask, f\"p{i}_2\"] = s_m * (1 - c_m)            # 成功 & 无代价\n",
    "    df_s_c.loc[mask, f\"p{i}_3\"] = (1 - s_m) * c_m            # 失败 & 代价\n",
    "    df_s_c.loc[mask, f\"p{i}_4\"] = (1 - s_m) * (1 - c_m)      # 失败 & 无代价\n",
    "\n",
    "    if check_sum:\n",
    "        rowsum = df_s_c[[f\"p{i}_1\", f\"p{i}_2\", f\"p{i}_3\", f\"p{i}_4\"]].sum(axis=1)\n",
    "        ok = (rowsum[mask] - 1.0).abs().le(tol)\n",
    "        if not ok.all():\n",
    "            bad = (~ok).sum()\n",
    "            raise ValueError(f\"第{i}组概率行和!=1 的行数: {bad}，请检查 s/c 是否已规范到[0,1] 或是否存在数据异常。\")\n",
    "\n",
    "add_prob_columns(df_s_c, 's_1', 'c_1', 1)\n",
    "add_prob_columns(df_s_c, 's_2', 'c_2', 2)\n",
    "add_prob_columns(df_s_c, 's_3', 'c_3', 3)\n",
    "add_prob_columns(df_s_c, 's_4', 'c_4', 4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ad9cca1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_1to5(series):\n",
    "    # 转为数值型，非法值→NaN\n",
    "    s = pd.to_numeric(series, errors=\"coerce\")\n",
    "    # 可选：裁剪到 1~5，防止意外越界\n",
    "    s = s.clip(1, 5)\n",
    "    return (s - 1) / 4.0\n",
    "\n",
    "alpha   = scale_1to5(data[\"3、你能在多大水平上承担选择可能带来的风险？\"])\n",
    "lambda1 = scale_1to5(data[\"4、你在做决策时，对风险的考虑在多大程度上影响你的决定？\"])\n",
    "lambda2 = scale_1to5(data[\"5、你在做决策时，多大程度上在意收益是否稳定？\"])\n",
    "\n",
    "df_alpha_lambda = pd.DataFrame({\n",
    "    \"alpha\": alpha,\n",
    "    \"lambda1\": lambda1,\n",
    "    \"lambda2\": lambda2\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "acfa604d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   H_alpha_1  H_alpha_2  H_alpha_3     H_alpha_4\n",
      "0   1.206727   0.392261   0.677050  3.615505e-11\n",
      "1   1.001505   1.156162   1.040547  4.754065e-01\n",
      "2   0.500402   1.193550   1.173414  3.250830e-01\n",
      "3   1.086760   1.054612   1.165809  1.010070e+00\n",
      "4   0.754059   0.759660   1.097667  8.963367e-01\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def fractional_entropy(df_probs: pd.DataFrame, alpha: pd.Series, eps: float = 1e-12):\n",
    "    # 1) alpha 数值化 + 裁剪 + 与 df_probs 行对齐\n",
    "    a = pd.to_numeric(alpha, errors=\"coerce\").clip(0, 1).reindex(df_probs.index)\n",
    "\n",
    "    def H_for(cols):\n",
    "        # 列存在性校验（也方便早报警）\n",
    "        missing = [c for c in cols if c not in df_probs.columns]\n",
    "        if missing:\n",
    "            raise KeyError(f\"缺少概率列: {missing}\")\n",
    "\n",
    "        # 2) 概率矩阵，保留 NaN；数值裁剪到 [eps, 1]\n",
    "        P = df_probs[cols].astype(float)\n",
    "        P = P.clip(lower=eps, upper=1.0)\n",
    "\n",
    "        # 3) 计算：H = Σ P * (-log P)^{alpha}\n",
    "        L = -np.log(P.to_numpy())          # (n, 4)\n",
    "        A = a.to_numpy().reshape(-1, 1)    # (n, 1) 广播到 (n, 4)\n",
    "        H = (P.to_numpy() * (L ** A)).sum(axis=1)\n",
    "\n",
    "        # 若某行初始有 NaN，clip 不会修复它；希望该行 H=NaN 的话，下行可保持默认\n",
    "        # 如果希望“忽略 NaN 的列再求和”，可用 np.nansum 代替 sum\n",
    "        return pd.Series(H, index=df_probs.index)\n",
    "\n",
    "    H = pd.DataFrame({\n",
    "        'H_alpha_1': H_for(['p1_1','p1_2','p1_3','p1_4']),\n",
    "        'H_alpha_2': H_for(['p2_1','p2_2','p2_3','p2_4']),\n",
    "        'H_alpha_3': H_for(['p3_1','p3_2','p3_3','p3_4']),\n",
    "        'H_alpha_4': H_for(['p4_1','p4_2','p4_3','p4_4']),\n",
    "    })\n",
    "\n",
    "    # 4) 基本数值自检（非负；允许 NaN）\n",
    "    assert ((H >= 0) | H.isna()).all().all(), \"存在负的熵值，请检查概率或 alpha。\"\n",
    "    return H\n",
    "\n",
    "H_alpha_df = fractional_entropy(df_s_c, df_alpha_lambda['alpha'])\n",
    "print(H_alpha_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "240ee254",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(df, columns_to_normalize):\n",
    "    for col in columns_to_normalize:\n",
    "        min_val = df[col].min()\n",
    "        max_val = df[col].max()\n",
    "        rng = max_val - min_val\n",
    "        if pd.isna(rng) or rng == 0:\n",
    "            df[col] = 0.0  # 所有值一样或全 NaN\n",
    "        else:\n",
    "            df[col] = (df[col] - min_val) / rng\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fc5798fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            alpha     lambda1     lambda2          C1          C2          C3  \\\n",
      "count  104.000000  104.000000  104.000000  104.000000  104.000000  104.000000   \n",
      "mean     0.600962    0.673077    0.668269    0.496102    0.617094    0.588287   \n",
      "std      0.296663    0.309786    0.314385    0.243663    0.205760    0.219832   \n",
      "min      0.000000    0.000000    0.000000    0.000000    0.000000    0.000000   \n",
      "25%      0.500000    0.500000    0.500000    0.317568    0.483333    0.431818   \n",
      "50%      0.500000    0.750000    0.750000    0.513514    0.644444    0.613636   \n",
      "75%      0.750000    1.000000    1.000000    0.675676    0.777778    0.727273   \n",
      "max      1.000000    1.000000    1.000000    1.000000    1.000000    1.000000   \n",
      "\n",
      "               C4  \n",
      "count  104.000000  \n",
      "mean     0.572009  \n",
      "std      0.206893  \n",
      "min      0.000000  \n",
      "25%      0.466667  \n",
      "50%      0.600000  \n",
      "75%      0.694444  \n",
      "max      1.000000  \n"
     ]
    }
   ],
   "source": [
    "def s1to6(series):\n",
    "    return pd.to_numeric(series, errors=\"coerce\").clip(1, 6)\n",
    "\n",
    "def sum_items(df, cols, min_valid=1):\n",
    "    X = pd.concat([s1to6(df[c]) for c in cols], axis=1)\n",
    "    valid = X.notna().sum(axis=1) >= min_valid\n",
    "    s = X.sum(axis=1)\n",
    "    s[~valid] = np.nan\n",
    "    return s\n",
    "\n",
    "# 公平价值 F_v（E 已删除）\n",
    "F_cols = [\n",
    "    \"9、在你选择的案例中，你对以下说法有多认可，1代表完全不同意，6代表完全同意—A．\\t相关人的行为损害了社会公正\",\n",
    "    \"9、B．\\t有些人没有得到他们应得的\",\n",
    "    \"9、C．\\t不公正战胜了公正\",\n",
    "    \"9、D．\\t遭受了不公正的人没有得到补偿\",\n",
    "    # \"9、E．\\t案例中的不公正是偶然的，而不是必然\",  # 已删除\n",
    "    \"9、F．\\t相关人员做决定时没有力求公正\",\n",
    "]\n",
    "F_v = sum_items(data, F_cols, min_valid=3)  # 例如至少答3题才计分\n",
    "\n",
    "# 忠诚成本 L_c\n",
    "L_cols = [\n",
    "    \"10、在你选择的案例中，你对以下说法有多认可，1代表完全不同意，6代表完全同意—A．\\t举报同事或朋友是一种对关系的背叛\",\n",
    "    \"10、B．\\t即使举报是正确的，也会削弱我对团队的忠诚感\",\n",
    "    \"10、C．\\t举报行为会被视为对团队利益的损害\",\n",
    "    \"10、D．\\t考虑到我和朋友、同事的关系，我会犹豫是否继续举报\",\n",
    "    \"10、E．\\t即使有不当行为，我仍会优先维护与团队的关系\",\n",
    "    \"10、F．\\t举报会让我感觉自己不再完全属于这个群体\",\n",
    "]\n",
    "L_c = sum_items(data, L_cols, min_valid=4)\n",
    "\n",
    "# 各行为的感知收益 P_k\n",
    "P1_cols = [\n",
    "    \"11、假设你即将要采取直接举报的行动，那么你对下面的说法认可程度多高，1代表完全不同意，6代表完全同意—A.\\t这会提高我在组织中的声望\",\n",
    "    \"11、B.\\t这会给我带来职业晋升机会\",\n",
    "    \"11、C.\\t这会使我获得满意的经济等物质奖励\",\n",
    "    \"11、D.\\t这会让我觉得我在做正确的事情\",\n",
    "    \"11、E.\\t这会使我获得心理上的满足或者释怀\",\n",
    "]\n",
    "P2_cols = [\n",
    "    \"12、假设你即将要采取匿名举报的行动，那么你对下面的说法认可程度多高，1代表完全不同意，6代表完全同意—A.\\t这会提高我在组织中的声望\",\n",
    "    \"12、B.\\t这会给我带来职业晋升机会\",\n",
    "    \"12、C.\\t这会使我获得满意的经济等物质奖励\",\n",
    "    \"12、D.\\t这会让我觉得我在做正确的事情\",\n",
    "    \"12、E.\\t这会使我获得心理上的满足或者释怀\",\n",
    "]\n",
    "P3_cols = [\n",
    "    \"13、假设你即将要采取继续保持观察，等待时机或者搜集更多证据后再采取行动的行动，那么你对下面的说法认可程度多高，1代表完全不同意，6代表完全同意—A.\\t这会提高我在组织中的声望\",\n",
    "    \"13、B.\\t这会给我带来职业晋升机会\",\n",
    "    \"13、C.\\t这会使我获得满意的经济等物质奖励\",\n",
    "    \"13、D.\\t这会让我觉得我在做正确的事情\",\n",
    "    \"13、E.\\t这会使我获得心理上的满足或者释怀\",\n",
    "]\n",
    "P4_cols = [\n",
    "    \"14、假设你即将要采取保持沉默，等待第三方想办法解决问题的行动，那么你对下面的说法认可程度多高，1代表完全不同意，6代表完全同意—A.\\t这会提高我在组织中的声望\",\n",
    "    \"14、B.\\t这会给我带来职业晋升机会\",\n",
    "    \"14、C.\\t这会使我获得满意的经济等物质奖励\",\n",
    "    \"14、D.\\t这会让我觉得我在做正确的事情\",\n",
    "    \"14、E.\\t这会使我获得心理上的满足或者释怀\",\n",
    "]\n",
    "\n",
    "P_1p = sum_items(data, P1_cols, min_valid=3)\n",
    "P_2p = sum_items(data, P2_cols, min_valid=3)\n",
    "P_3p = sum_items(data, P3_cols, min_valid=3)\n",
    "P_4p = sum_items(data, P4_cols, min_valid=3)\n",
    "\n",
    "# 每一种行为的综合效用（方向：公平↑、忠诚成本↓、感知收益↑）\n",
    "C1 = F_v - L_c + P_1p\n",
    "C2 = F_v - L_c + P_2p\n",
    "C3 = F_v - L_c + P_3p\n",
    "C4 = F_v - L_c + P_4p\n",
    "\n",
    "df_ci = pd.DataFrame({\"C1\": C1, \"C2\": C2, \"C3\": C3, \"C4\": C4})\n",
    "\n",
    "# 列内 Min–Max 归一化到 [0,1]\n",
    "df_ci = normalize(df_ci, ['C1', 'C2', 'C3', 'C4'])\n",
    "\n",
    "# 汇总到 data2\n",
    "data2 = pd.concat([df_alpha_lambda, df_ci], axis=1)\n",
    "print(data2.describe())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3a5642b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   alpha        C1        C2        C3        C4        U1        U2  \\\n",
      "0   0.75  0.108108  0.200000  0.136364  0.244444  0.061990  0.133748   \n",
      "1   0.50  0.270270  0.444444  0.386364  0.444444  0.270270  0.444444   \n",
      "2   1.00  0.513514  0.600000  0.545455  0.488889  0.367983  0.464758   \n",
      "3   0.50  0.162162  0.311111  0.272727  0.222222  0.162162  0.311111   \n",
      "4   0.75  0.459459  0.555556  0.409091  0.422222  0.378276  0.479633   \n",
      "\n",
      "         U3        U4  \n",
      "0  0.082865  0.171880  \n",
      "1  0.386364  0.444444  \n",
      "2  0.402845  0.341834  \n",
      "3  0.272727  0.222222  \n",
      "4  0.327171  0.340350  \n",
      "   alpha  lambda1  lambda2        C1        C2        C3        C4        U1  \\\n",
      "0   0.75     0.50     0.50  0.108108  0.200000  0.136364  0.244444  0.061990   \n",
      "1   0.50     0.75     0.25  0.270270  0.444444  0.386364  0.444444  0.270270   \n",
      "2   1.00     1.00     1.00  0.513514  0.600000  0.545455  0.488889  0.367983   \n",
      "3   0.50     0.75     1.00  0.162162  0.311111  0.272727  0.222222  0.162162   \n",
      "4   0.75     1.00     1.00  0.459459  0.555556  0.409091  0.422222  0.378276   \n",
      "\n",
      "         U2        U3        U4  H_alpha_1  H_alpha_2  H_alpha_3     H_alpha_4  \n",
      "0  0.133748  0.082865  0.171880   1.206727   0.392261   0.677050  3.615505e-11  \n",
      "1  0.444444  0.386364  0.444444   1.001505   1.156162   1.040547  4.754065e-01  \n",
      "2  0.464758  0.402845  0.341834   0.500402   1.193550   1.173414  3.250830e-01  \n",
      "3  0.311111  0.272727  0.222222   1.086760   1.054612   1.165809  1.010070e+00  \n",
      "4  0.479633  0.327171  0.340350   0.754059   0.759660   1.097667  8.963367e-01  \n"
     ]
    }
   ],
   "source": [
    "# 计算效用函数（平滑风险偏好）\n",
    "# 假定 data2 里有列：alpha, C1..C4，且 Ck ∈ [0,1]\n",
    "def utility_power(C_series: pd.Series, alpha_series: pd.Series):\n",
    "    a = pd.to_numeric(alpha_series, errors=\"coerce\").clip(0, 1).reindex(C_series.index)\n",
    "    beta = 0.5 + a  # beta ∈ [0.5, 1.5]\n",
    "    # 对齐索引逐元素幂运算；有 NaN 的地方保留 NaN\n",
    "    return C_series.astype(float) ** beta\n",
    "\n",
    "for i, col in enumerate(['C1', 'C2', 'C3', 'C4'], start=1):\n",
    "    data2[f\"U{i}\"] = utility_power(data2[col], data2['alpha'])\n",
    "\n",
    "print(data2[['alpha','C1','C2','C3','C4','U1','U2','U3','U4']].head())\n",
    "\n",
    "# 合并熵项\n",
    "data3 = pd.concat([data2, H_alpha_df], axis=1)\n",
    "print(data3.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6c557efa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         U1        U2        U3        U4  H_alpha_1  H_alpha_2  H_alpha_3  \\\n",
      "0  0.061990  0.133748  0.082865  0.171880   1.206727   0.392261   0.677050   \n",
      "1  0.270270  0.444444  0.386364  0.444444   1.001505   1.156162   1.040547   \n",
      "2  0.367983  0.464758  0.402845  0.341834   0.500402   1.193550   1.173414   \n",
      "3  0.162162  0.311111  0.272727  0.222222   1.086760   1.054612   1.165809   \n",
      "4  0.378276  0.479633  0.327171  0.340350   0.754059   0.759660   1.097667   \n",
      "\n",
      "      H_alpha_4        R1        R2        R3        R4  \n",
      "0  3.615505e-11  0.968661  0.680127  0.835602  0.579829  \n",
      "1  4.754065e-01  0.893332  0.996143  0.936240  0.757810  \n",
      "2  3.250830e-01  0.831772  0.896398  0.939335  0.660965  \n",
      "3  1.010070e+00  0.953210  0.831432  0.926957  0.795913  \n",
      "4  8.963367e-01  0.880932  0.779289  0.923505  0.803541  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zimin\\AppData\\Local\\Temp\\ipykernel_26912\\2828293706.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[col] = (df[col] - min_val) / rng\n"
     ]
    }
   ],
   "source": [
    "# --- 1) 取列名 ---\n",
    "U_cols = ['U1', 'U2', 'U3', 'U4']\n",
    "H_cols = ['H_alpha_1', 'H_alpha_2', 'H_alpha_3', 'H_alpha_4']\n",
    "\n",
    "# --- 2) 三项统一到 [0,1] 标度 ---\n",
    "# 2.1 逐列 Min–Max 归一 U（这是行级数据）\n",
    "U_norm = data3[U_cols].copy()\n",
    "U_norm = normalize(U_norm, U_cols)  # 逐列到[0,1]\n",
    "\n",
    "# 2.2 逐列 Min–Max 归一 H（这是行级数据）\n",
    "H_norm = data3[H_cols].copy()\n",
    "H_norm = normalize(H_norm, H_cols)\n",
    "\n",
    "# 2.3 S：沿用你“列标准差”的思路（常量），再把4个常量缩放到 [0,1]\n",
    "S_series = data3[U_cols].std(axis=0, ddof=0)  # 每个行为一值（列常量）\n",
    "S_min, S_max = S_series.min(), S_series.max()\n",
    "den = (S_max - S_min) if pd.notna(S_max - S_min) and (S_max - S_min) != 0 else 1.0\n",
    "S_norm_scalar = (S_series - S_min) / den       # U1..U4 各一个标量\n",
    "\n",
    "# --- 3) 行级权重（非负 + 归一化到和=1） ---\n",
    "lam1 = data3['lambda1'].clip(0, 1)\n",
    "lam2 = data3['lambda2'].clip(0, 1)\n",
    "w3_raw = 1.0 - lam1 - lam2\n",
    "w1 = lam1.clip(lower=0)         # 防止负\n",
    "w2 = lam2.clip(lower=0)\n",
    "w3 = w3_raw.clip(lower=0)\n",
    "\n",
    "w_sum = (w1 + w2 + w3).replace(0, np.nan)  # 避免除零\n",
    "w1n = w1 / w_sum\n",
    "w2n = w2 / w_sum\n",
    "w3n = w3 / w_sum\n",
    "\n",
    "# 若某行 w_sum 为 NaN（极少数：λ1=λ2=0 且被裁剪导致全0），则默认给 U 项全部权重\n",
    "w1n = w1n.fillna(0.0)\n",
    "w2n = w2n.fillna(0.0)\n",
    "w3n = w3n.fillna(1.0)\n",
    "\n",
    "# --- 4) 组合打分 R1..R4（行级计算） ---\n",
    "for i in range(4):\n",
    "    U_col = U_cols[i]\n",
    "    H_col = H_cols[i]\n",
    "    S_scalar = float(S_norm_scalar[U_col])  # 标量\n",
    "\n",
    "    # 注意三项方向：不确定性(H) ↑ 不好 → 加权加；离散度(S) ↑ 不好 → 加权加；\n",
    "    # 收益(U) ↑ 好 → 加权“减去其负效应”相当于 +w3n*U 的好处（这里写成 -(-U) 更直观）\n",
    "    # 统一到 R 是“代价/风险分数”，值越小越好（也可取相反号做“效用分数”）\n",
    "    data3[f'R{i+1}'] = (\n",
    "        w1n * H_norm[H_col]        # 熵的惩罚\n",
    "        + w2n * S_scalar           # 方差的惩罚（常量）\n",
    "        - w3n * U_norm[U_col]      # 收益的加分（以减法方式进入）\n",
    "    )\n",
    "\n",
    "# 可选：最终再把 R1..R4 逐列压回 [0,1] 便于比较/可视化\n",
    "R_cols = [f'R{i}' for i in range(1,5)]\n",
    "data3[R_cols] = normalize(data3[R_cols], R_cols)\n",
    "\n",
    "print(data3[[*U_cols, *H_cols, *R_cols]].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "465ddee8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ 未找到‘最终选择’列。请在下列接近的列中确认：\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'decision_int'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32mD:\\anaconda\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3653\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3652\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3653\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[0;32m   3654\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32mD:\\anaconda\\Lib\\site-packages\\pandas\\_libs\\index.pyx:147\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mD:\\anaconda\\Lib\\site-packages\\pandas\\_libs\\index.pyx:176\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:7080\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:7088\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'decision_int'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 93\u001b[0m\n\u001b[0;32m     90\u001b[0m     data3[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfinal_choice\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m final_choice\n\u001b[0;32m     92\u001b[0m \u001b[38;5;66;03m# —— 后续评估：只在 decision 与 final_choice 都非缺失的样本上计算 —— \u001b[39;00m\n\u001b[1;32m---> 93\u001b[0m mask_eval \u001b[38;5;241m=\u001b[39m data3[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdecision_int\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mnotna() \u001b[38;5;241m&\u001b[39m data3[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfinal_choice\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mnotna()\n\u001b[0;32m     94\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m用于评估的样本数：\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mint\u001b[39m(mask_eval\u001b[38;5;241m.\u001b[39msum()))\n\u001b[0;32m     96\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mask_eval\u001b[38;5;241m.\u001b[39many():\n",
      "File \u001b[1;32mD:\\anaconda\\Lib\\site-packages\\pandas\\core\\frame.py:3761\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   3760\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 3761\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mget_loc(key)\n\u001b[0;32m   3762\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   3763\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[1;32mD:\\anaconda\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3655\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3653\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[0;32m   3654\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m-> 3655\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3656\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3657\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3658\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3659\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3660\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'decision_int'"
     ]
    }
   ],
   "source": [
    "# —— 模型模拟选择：找每行最小 R 的列（1~4）\n",
    "R_cols = ['R1','R2','R3','R4']\n",
    "vals = data3[R_cols].to_numpy(dtype=float)\n",
    "\n",
    "# 用 +inf 代替 NaN，这样 argmin 不会报错；全 NaN 的行依然会得到 +inf\n",
    "vals_safe = np.where(np.isnan(vals), np.inf, vals)\n",
    "idx0 = vals_safe.argmin(axis=1)           # 0-based\n",
    "R_min = vals_safe[np.arange(len(vals_safe)), idx0]\n",
    "\n",
    "# 标记全 NaN 的行：四列全是 NaN -> vals_safe 一整行都是 inf\n",
    "all_nan = np.isinf(vals_safe).all(axis=1)\n",
    "idx0[all_nan] = -1\n",
    "R_min[all_nan] = np.nan\n",
    "\n",
    "data3['R_min'] = R_min\n",
    "data3['decision'] = np.where(idx0 >= 0, idx0 + 1, np.nan)  # 1..4\n",
    "\n",
    "# —— 真实最终选择清洗为 1..4\n",
    "# 若原始是数字 1-4：to_numeric 后直接用；若是中文选项：请按你的问卷选项映射\n",
    "choice_raw = data[\"15、你最后会选择怎么办？\"]\n",
    "\n",
    "# 先尝试数值；如果是中文请改用下面的 mapping\n",
    "y_true_num = pd.to_numeric(choice_raw, errors='coerce')\n",
    "\n",
    "# 如果你的真实值是中文，请取消注释并按你的选项文字改：\n",
    "# mapping = {\n",
    "#     \"直接举报\": 1,\n",
    "#     \"匿名举报\": 2,\n",
    "#     \"继续保持观察，等待时机或者搜集更多证据后再采取行动\": 3,\n",
    "#     \"保持沉默，等待第三方想办法解决问题\": 4\n",
    "# }\n",
    "# y_true_num = choice_raw.map(mapping)\n",
    "\n",
    "data3[\"final_choice\"] = y_true_num\n",
    "\n",
    "# —— 评估：丢掉 y_true 或 y_pred 为 NaN 的行再计算\n",
    "valid_mask = data3['final_choice'].notna() & data3['decision'].notna()\n",
    "y_true = data3.loc[valid_mask, 'final_choice'].astype(int)\n",
    "y_pred = data3.loc[valid_mask, 'decision'].astype(int)\n",
    "\n",
    "print(f\"用于评估的有效样本数: {len(y_true)} / 总样本: {len(data3)}\")\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "labels = [1, 2, 3, 4]\n",
    "cm = confusion_matrix(y_true, y_pred, labels=labels)\n",
    "cm_df = pd.DataFrame(cm, index=[f'True {l}' for l in labels],\n",
    "                        columns=[f'Pred {l}' for l in labels])\n",
    "\n",
    "print(\"混淆矩阵：\\n\", cm_df)\n",
    "print(\"\\n分类报告：\")\n",
    "print(classification_report(y_true, y_pred, labels=labels, zero_division=0))\n",
    "acc = accuracy_score(y_true, y_pred)\n",
    "print(f\"总体准确率: {acc:.4f}\")\n",
    "\n",
    "# 可视化（彩色热力图）\n",
    "plt.figure(figsize=(6, 5))\n",
    "sns.heatmap(cm_df, annot=True, fmt='d', cmap='Blues', cbar=True)\n",
    "plt.title(f'Confusion Matrix (Accuracy={acc:.2%})', fontsize=14)\n",
    "plt.ylabel('True label')\n",
    "plt.xlabel('Predicted label')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36c86ac5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
